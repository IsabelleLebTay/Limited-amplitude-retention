{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply Distance Truncation to Acoustic Survey Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```yaml\n",
    "---\n",
    "title: \"Apply Distance Truncation to Acoustic Survey Data\"\n",
    "description: >\n",
    "  This notebook demonstrates the process of applying perceptibility truncation \n",
    "  to acoustic survey data for bird species. It includes steps for loading \n",
    "  libraries and data, creating distance-based amplitude dictionaries, filtering \n",
    "  data based on predicted amplitudes, and processing species counts. The \n",
    "  truncation method adjusts detection distances based on species, habitat type, \n",
    "  and recording equipment, improving the accuracy of abundance estimates from \n",
    "  acoustic surveys.\n",
    "author: \"Isabelle Lebeuf-Taylor\"\n",
    "date: \"2024-09-25\"\n",
    "tags:\n",
    "  - acoustic surveys\n",
    "  - distance truncation\n",
    "---\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load libraries and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load predictive dataframe from the attenuation model\n",
    "counts_to_truncate = pd.read_csv(\"Data/counts_to_truncate.csv\")\n",
    "predicted_amps = pd.read_csv(\"Data/predicted_distance_amplitudes.csv\")\n",
    "number_of_visits_per_site = pd.read_csv(\"Data/number_of_transcribed_recordings_per_site.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Apply truncation to real data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Dictionary of distances and predicted amplitudes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a dictionary of dictionaries where the key is the distance (integer) and the value is the dictionary of the open and forested amps at which to truncate for a given distance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Target species  | Reference |\n",
    "|------|-------|\n",
    "| WTSP | VESP  |\n",
    "| RCKI | AMRO  |\n",
    "| TEWA | AMRO  |\n",
    "| OSFL | WEME  |\n",
    "| YRWA | CCSP  |\n",
    "| REVI | BRBL  |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference species for my focal species\n",
    "spp_dict = {\n",
    "    \"WTSP\": \"VESP\",\n",
    "    \"RCKI\": \"AMRO\",\n",
    "    \"TEWA\": \"AMRO\",\n",
    "    \"OSFL\": \"WEME\",\n",
    "    \"YRWA\": \"CCSP\",\n",
    "    \"REVI\": \"BRBL\"\n",
    "}\n",
    "\n",
    "def map_spp(reference_spp):\n",
    "    for key, value in spp_dict.items():\n",
    "        if reference_spp == value:\n",
    "            return key\n",
    "    return None \n",
    "\n",
    "predicted_amps['spp'] = predicted_amps['species_code'].apply(map_spp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_distance_dict(predicted_amps):\n",
    "    \"\"\"\n",
    "    Create a nested dictionary of predicted amplitudes for species of interest across different distances,\n",
    "    forest types, and SM2 conditions.\n",
    "\n",
    "    Args:\n",
    "    predicted_amps (DataFrame): A DataFrame containing predicted amplitudes and associated metadata.\n",
    "\n",
    "    Returns:\n",
    "    dict: A nested dictionary with the structure:\n",
    "          {distance: {species: {'OP': {'SM2_0': value, 'SM2_1': value},\n",
    "                                'FO': {'SM2_0': value, 'SM2_1': value}}}}\n",
    "    \"\"\"\n",
    "\n",
    "    # TEWA and RCI are the same, so use RCKI fro the TEWA part\n",
    "    species_of_interest = ['RCKI', 'WTSP', 'YRWA', 'REVI', 'OSFL']\n",
    "\n",
    "    # Filter DataFrames for 'BinForest' conditions with the addition of 'SM2' filtering\n",
    "    df_op_0 = predicted_amps[(predicted_amps['BinForest'] == 'OP') & (predicted_amps['SM2'] == 0)]\n",
    "    df_op_1 = predicted_amps[(predicted_amps['BinForest'] == 'OP') & (predicted_amps['SM2'] == 1)]\n",
    "    df_fo_0 = predicted_amps[(predicted_amps['BinForest'] == 'FO') & (predicted_amps['SM2'] == 0)]\n",
    "    df_fo_1 = predicted_amps[(predicted_amps['BinForest'] == 'FO') & (predicted_amps['SM2'] == 1)]\n",
    "\n",
    "    # Initialize the main dictionary\n",
    "    distance_dict = {}\n",
    "\n",
    "    for distance in range(30, 500):\n",
    "        distance_dict[distance] = {}\n",
    "        for species in species_of_interest:\n",
    "            # Initialize nested dictionaries for this species\n",
    "            distance_dict[distance][species] = {'OP': {'SM2_0': None, 'SM2_1': None},\n",
    "                                                'FO': {'SM2_0': None, 'SM2_1': None}}\n",
    "\n",
    "            op_rows_0 = df_op_0[df_op_0['spp'] == species]\n",
    "            if not op_rows_0.empty:\n",
    "                closest_op_row_0 = op_rows_0.iloc[(op_rows_0['distance'] - distance).abs().argsort()[:1]]\n",
    "                closest_op_distance_0 = closest_op_row_0['distance'].values[0]\n",
    "                if abs(closest_op_distance_0 - distance) <= 1:\n",
    "                    distance_dict[distance][species]['OP']['SM2_0'] = closest_op_row_0['predicted'].values[0]\n",
    "\n",
    "            op_rows_1 = df_op_1[df_op_1['spp'] == species]\n",
    "            if not op_rows_1.empty:\n",
    "                closest_op_row_1 = op_rows_1.iloc[(op_rows_1['distance'] - distance).abs().argsort()[:1]]\n",
    "                closest_op_distance_1 = closest_op_row_1['distance'].values[0]\n",
    "                if abs(closest_op_distance_1 - distance) <= 1:\n",
    "                    distance_dict[distance][species]['OP']['SM2_1'] = closest_op_row_1['predicted'].values[0]\n",
    "\n",
    "            fo_rows_0 = df_fo_0[df_fo_0['spp'] == species]\n",
    "            if not fo_rows_0.empty:\n",
    "                closest_fo_row_0 = fo_rows_0.iloc[(fo_rows_0['distance'] - distance).abs().argsort()[:1]]\n",
    "                closest_fo_distance_0 = closest_fo_row_0['distance'].values[0]\n",
    "                if abs(closest_fo_distance_0 - distance) <= 1:\n",
    "                    distance_dict[distance][species]['FO']['SM2_0'] = closest_fo_row_0['predicted'].values[0]\n",
    "\n",
    "            fo_rows_1 = df_fo_1[df_fo_1['spp'] == species]\n",
    "            if not fo_rows_1.empty:\n",
    "                closest_fo_row_1 = fo_rows_1.iloc[(fo_rows_1['distance'] - distance).abs().argsort()[:1]]\n",
    "                closest_fo_distance_1 = closest_fo_row_1['distance'].values[0]\n",
    "                if abs(closest_fo_distance_1 - distance) <= 1:\n",
    "                    distance_dict[distance][species]['FO']['SM2_1'] = closest_fo_row_1['predicted'].values[0]\n",
    "\n",
    "    for distance, species_dict in distance_dict.items():\n",
    "        if 'RCKI' in species_dict:\n",
    "            species_dict['TEWA'] = species_dict['RCKI']\n",
    "\n",
    "    return distance_dict\n",
    "\n",
    "distance_dict = create_distance_dict(predicted_amps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Filter predicted data by distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_dataframes_by_distance(dataframe, distance_dicts):\n",
    "    \"\"\"\n",
    "    Filter a dataframe based on distance-specific criteria.\n",
    "\n",
    "    Args:\n",
    "    dataframe (pd.DataFrame): The input dataframe to be filtered.\n",
    "    distance_dicts (dict): A dictionary containing filtering criteria for each distance.\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary where keys are distances and values are filtered dataframes.\n",
    "    \"\"\"\n",
    "    filtered_dfs = {}\n",
    "    \n",
    "    for distance in range(30, 501): \n",
    "        filter_dict = distance_dicts.get(distance)\n",
    "        if filter_dict is not None: \n",
    "            all_filtered_rows = []  \n",
    "            for species, habitats in filter_dict.items():\n",
    "                for habitat_type, sm2_values in habitats.items():\n",
    "                    for sm2, amp_threshold in sm2_values.items():\n",
    "                        year_condition = (dataframe['Year_since_logging'] < 11) if habitat_type == 'OP' else (dataframe['Year_since_logging'] >= 12)\n",
    "                        sm2_condition = (dataframe['SM2'] == float(sm2.split('_')[-1]))\n",
    "                        amp_condition = (dataframe['mean_amp'] >= amp_threshold) if amp_threshold is not None else pd.Series([True] * len(dataframe))\n",
    "                        condition = (dataframe['species_code'] == species) & year_condition & sm2_condition & amp_condition\n",
    "                        \n",
    "                        filtered_rows = dataframe[condition]\n",
    "                        if not filtered_rows.empty:\n",
    "                            all_filtered_rows.append(filtered_rows)\n",
    "            \n",
    "            if all_filtered_rows:\n",
    "                filtered_df = pd.concat(all_filtered_rows, ignore_index=True)\n",
    "                filtered_dfs[distance] = filtered_df\n",
    "    \n",
    "    return filtered_dfs\n",
    "\n",
    "\n",
    "filtered_dfs = filter_dataframes_by_distance(counts_to_truncate, distance_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_dfs = filter_dataframes_by_distance(counts_to_truncate, distance_dict)\n",
    "# filtered_dfs[150].to_csv('Truncated_150m.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check that the filtering behaves as expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity check passed! The filtered dataframe meets all conditions.\n"
     ]
    }
   ],
   "source": [
    "def sanity_check(dataframe, filter_dict):\n",
    "    \"\"\"\n",
    "    Perform a sanity check on a filtered dataframe against given filter conditions.\n",
    "\n",
    "    This function verifies if all rows in the dataframe meet the specified amplitude thresholds for each species, \n",
    "    considering different thresholds based on the years since logging and SM2 status.\n",
    "\n",
    "    Args:\n",
    "    dataframe (pd.DataFrame): The filtered dataframe to check. Must contain columns 'species_code', 'Year_since_logging', 'SM2', and 'mean_amp'.\n",
    "    \n",
    "    filter_dict (dict): A nested dictionary with the structure:\n",
    "                        {species: {'OP': {'SM2_0': value, 'SM2_1': value},\n",
    "                                   'FO': {'SM2_0': value, 'SM2_1': value}}}\n",
    "\n",
    "    Returns:\n",
    "    str: A message indicating whether the sanity check passed or failed. If failed, \n",
    "         it specifies which species and condition caused the failure.\n",
    "\n",
    "    Note:\n",
    "    The function assumes that 'Year_since_logging' < 11 corresponds to recently logged areas (OP),\n",
    "    and >= 12 to older logged or unlogged areas (FO). 'SM2' can be 0 or 1.\n",
    "    \"\"\"\n",
    "    for species, conditions in filter_dict.items():\n",
    "        species_df = dataframe[dataframe['species_code'] == species]\n",
    "        \n",
    "        for sm2_status in [0, 1]:\n",
    "            # Check for recently logged areas (OP)\n",
    "            invalid_rows_op = species_df[\n",
    "                (species_df['Year_since_logging'] < 11) & \n",
    "                (species_df['SM2'] == sm2_status) & \n",
    "                (species_df['mean_amp'] < conditions['OP'][f'SM2_{sm2_status}'])\n",
    "            ]\n",
    "            \n",
    "            # Check for older logged or unlogged areas (FO)\n",
    "            invalid_rows_fo = species_df[\n",
    "                (species_df['Year_since_logging'] >= 12) & \n",
    "                (species_df['SM2'] == sm2_status) & \n",
    "                (species_df['mean_amp'] < conditions['FO'][f'SM2_{sm2_status}'])\n",
    "            ]\n",
    "            \n",
    "            if not invalid_rows_op.empty:\n",
    "                return f\"Sanity check failed for species {species}, recently logged areas (OP), SM2 {sm2_status}. Check the filter conditions and data.\"\n",
    "            \n",
    "            if not invalid_rows_fo.empty:\n",
    "                return f\"Sanity check failed for species {species}, older logged/unlogged areas (FO), SM2 {sm2_status}. Check the filter conditions and data.\"\n",
    "    \n",
    "    return \"Sanity check passed! The filtered dataframe meets all conditions.\"\n",
    "\n",
    "sanity_check_result = sanity_check(filtered_dfs[150], distance_dict[150])\n",
    "print(sanity_check_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Apply filtering to real data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcribed_tasks_all = pd.read_csv(\"Data/transcribed_tasks_all.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Exclude songs that are quieter than the expected amplitude given a distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_species_counts(filtered_dfs, transcribed_tasks_all, distance, species_of_interest=None):\n",
    "    \"\"\"\n",
    "    Process species counts data for a given distance, filtering and combining data from multiple sources.\n",
    "\n",
    "    Args:\n",
    "    filtered_dfs (dict): Dictionary of filtered dataframes for different distances.\n",
    "    transcribed_tasks_all (pd.DataFrame): Dataframe containing all transcribed tasks.\n",
    "    distance (int): The distance (in meters) for which to process the data.\n",
    "    species_of_interest (list, optional): List of species codes to include. Defaults to ['TEWA', 'RCKI', 'WTSP', 'YRWA', 'REVI', 'OSFL'].\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: Processed dataframe with species counts for each location and recording date/time.\n",
    "\n",
    "    Note:\n",
    "    - Removes specific sites: \"H23-RS-167\", \"H-RS-1-98\", \"H23-RS-109\": these have retnetion ptaches larger than 12,000 m^2\n",
    "    - Filters for locations with 10 or more visits.\n",
    "    \"\"\"\n",
    "    if species_of_interest is None:\n",
    "        species_of_interest = ['TEWA', 'RCKI', 'WTSP', 'YRWA', 'REVI', 'OSFL']\n",
    "\n",
    "    # Read and filter sites\n",
    "    list_of_sites_to_use = number_of_visits_per_site[number_of_visits_per_site['number_of_visits'] >= 10]['location'].tolist()\n",
    "    list_of_sites_to_use = [site for site in list_of_sites_to_use if site not in [\"H23-RS-167\", \"H-RS-1-98\", \"H23-RS-109\"]]\n",
    "\n",
    "    # Group and pivot\n",
    "    grouped_all_df = filtered_dfs[distance].groupby(['location', 'recording_date_time', 'species_code']).size().reset_index(name='count')\n",
    "    pivot_all_df = grouped_all_df.pivot_table(index=['location', 'recording_date_time'], columns='species_code', values='count', fill_value=0).reset_index()\n",
    "    pivot_all_df = pivot_all_df[['location', 'recording_date_time'] + species_of_interest].fillna(0)\n",
    "\n",
    "    # Check for missing combinations\n",
    "    transcribed_combinations = set(transcribed_tasks_all[['location', 'recording_date_time']].apply(tuple, axis=1))\n",
    "    existing_combinations = set(pivot_all_df[['location', 'recording_date_time']].apply(tuple, axis=1))\n",
    "    missing_combinations = transcribed_combinations - existing_combinations\n",
    "\n",
    "    # Create dataframe for missing combinations\n",
    "    missing_combinations_df = pd.DataFrame(list(missing_combinations), columns=['location', 'recording_date_time'])\n",
    "    for species in species_of_interest:\n",
    "        missing_combinations_df[species] = 0\n",
    "\n",
    "    # Combine existing and missing data\n",
    "    updated_pivot_df = pd.concat([pivot_all_df, missing_combinations_df], ignore_index=True)\n",
    "\n",
    "    # Filter the final result\n",
    "    truncated_counts = updated_pivot_df[updated_pivot_df[['location', 'recording_date_time']].apply(tuple, axis=1).isin(transcribed_combinations)]\n",
    "\n",
    "    return truncated_counts\n",
    "\n",
    "\n",
    "truncated_150m_counts = process_species_counts(filtered_dfs, transcribed_tasks_all, 150)\n",
    "truncated_250m_counts = process_species_counts(filtered_dfs, transcribed_tasks_all, 250)\n",
    "\n",
    "truncated_150m_counts = pd.read_csv(\"Data/Abundance within 150m.csv\")\n",
    "truncated_250m_counts = pd.read_csv(\"Data/Abundance within 250m.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
